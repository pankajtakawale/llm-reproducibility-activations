{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3f16f33",
   "metadata": {},
   "source": [
    "# LLM Reproducibility with Activation Functions\n",
    "\n",
    "This notebook conducts reproducibility experiments on a character-level language model (Shakespeare dataset).\n",
    "\n",
    "**Experiment Design:**\n",
    "- Train multiple models with identical hyperparameters but different activation functions\n",
    "- Test: SmeLU (β=0.5, 1.0), ReLU, GELU, Swish\n",
    "- Measure reproducibility using Relative Prediction Difference (PD)\n",
    "- Compare accuracy vs reproducibility trade-offs\n",
    "\n",
    "**Expected Runtime:** ~2-3 hours for all experiments on M4 Pro CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507f3f2c",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d9327f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Local imports\n",
    "from config import Config\n",
    "from prepare_data import load_shakespeare, prepare_data\n",
    "from tokenizer import CharTokenizer\n",
    "from activations import get_activation\n",
    "from model import CharLM\n",
    "from train import run_experiment, set_seed\n",
    "\n",
    "# Set up matplotlib\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {torch.device('cpu')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42660278",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8d3951f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Configuration:\n",
      "  Model: 6 layers, 384 hidden dim, 6 heads\n",
      "  Context length: 256 characters\n",
      "  Training iterations: 5000\n",
      "  Batch size: 64\n",
      "  Learning rate: 0.0003\n",
      "  Trials per activation: 3\n",
      "  Activation functions: ['smelu_05', 'smelu_1', 'relu', 'gelu', 'swish']\n",
      "\n",
      "Config(model=6L-384H, iters=5000, batch=64)\n"
     ]
    }
   ],
   "source": [
    "# Initialize config\n",
    "config = Config()\n",
    "\n",
    "# Display configuration\n",
    "print(\"Experiment Configuration:\")\n",
    "print(f\"  Model: {config.n_layer} layers, {config.n_embd} hidden dim, {config.n_head} heads\")\n",
    "print(f\"  Context length: {config.block_size} characters\")\n",
    "print(f\"  Training iterations: {config.max_iters}\")\n",
    "print(f\"  Batch size: {config.batch_size}\")\n",
    "print(f\"  Learning rate: {config.learning_rate}\")\n",
    "print(f\"  Trials per activation: {config.trials_per_activation}\")\n",
    "print(f\"  Activation functions: {list(config.activation_functions.keys())}\")\n",
    "print(f\"\\n{config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bd4825",
   "metadata": {},
   "source": [
    "## Quick Test: Single Model Training\n",
    "\n",
    "First, test the setup by training a single model with ReLU activation (~5-10 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a01c686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing setup with single model...\n",
      "\n",
      "[INFO] Downloading Shakespeare dataset from https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "[INFO] Downloaded 1,115,394 characters to data/shakespeare.txt\n",
      "[INFO] Train size: 1,003,854 characters\n",
      "[INFO] Val size: 111,540 characters\n",
      "[INFO] Vocabulary size: 65\n",
      "[INFO] Characters: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "\n",
      "Vocabulary size: 65\n",
      "Train data: 1,003,854 characters\n",
      "Val data: 111,540 characters\n",
      "[INFO] Downloaded 1,115,394 characters to data/shakespeare.txt\n",
      "[INFO] Train size: 1,003,854 characters\n",
      "[INFO] Val size: 111,540 characters\n",
      "[INFO] Vocabulary size: 65\n",
      "[INFO] Characters: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "\n",
      "Vocabulary size: 65\n",
      "Train data: 1,003,854 characters\n",
      "Val data: 111,540 characters\n"
     ]
    }
   ],
   "source": [
    "# Test with single model\n",
    "print(\"Testing setup with single model...\\n\")\n",
    "\n",
    "# Load data\n",
    "text = load_shakespeare()\n",
    "train_text, val_text = prepare_data(text, config.train_split)\n",
    "tokenizer = CharTokenizer(text)\n",
    "config.vocab_size = len(tokenizer)\n",
    "\n",
    "train_data = tokenizer.encode(train_text)\n",
    "val_data = tokenizer.encode(val_text)\n",
    "\n",
    "print(f\"\\nVocabulary size: {config.vocab_size}\")\n",
    "print(f\"Train data: {len(train_data):,} characters\")\n",
    "print(f\"Val data: {len(val_data):,} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dcfc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Model initialized with 10,795,776 parameters\n",
      "\n",
      "============================================================\n",
      "Training Trial 0\n",
      "============================================================\n",
      "Step     0 | Train loss: 4.3444 | Val loss: 4.3433 | Time: 417.0s\n",
      "Step     0 | Train loss: 4.3444 | Val loss: 4.3433 | Time: 417.0s\n",
      "Step   500 | Train loss: 1.7497 | Val loss: 1.9022 | Time: 78114.4s\n",
      "Step   500 | Train loss: 1.7497 | Val loss: 1.9022 | Time: 78114.4s\n",
      "Step  1000 | Train loss: 1.4091 | Val loss: 1.6333 | Time: 131029.1s\n",
      "Step  1000 | Train loss: 1.4091 | Val loss: 1.6333 | Time: 131029.1s\n"
     ]
    }
   ],
   "source": [
    "# Create and train a test model\n",
    "from train import train_model\n",
    "\n",
    "set_seed(config.seed_base)\n",
    "test_model = CharLM(\n",
    "    vocab_size=config.vocab_size,\n",
    "    n_embd=config.n_embd,\n",
    "    n_head=config.n_head,\n",
    "    n_layer=config.n_layer,\n",
    "    block_size=config.block_size,\n",
    "    activation=get_activation('relu'),\n",
    "    dropout=config.dropout\n",
    ").to(config.device)\n",
    "\n",
    "test_results = train_model(test_model, train_data, val_data, config, trial_id=0)\n",
    "print(\"\\n✓ Test training completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bff267b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test text generation\n",
    "set_seed(42)\n",
    "test_model.eval()\n",
    "\n",
    "# Generate from a prompt\n",
    "prompt = \"ROMEO:\"\n",
    "context = torch.tensor(tokenizer.encode(prompt), dtype=torch.long).unsqueeze(0).to(config.device)\n",
    "generated = test_model.generate(context, max_new_tokens=200, temperature=0.8)\n",
    "generated_text = tokenizer.decode(generated[0].tolist())\n",
    "\n",
    "print(\"Generated text:\")\n",
    "print(\"=\" * 60)\n",
    "print(generated_text)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126ffd6d",
   "metadata": {},
   "source": [
    "## Full Experiments: All Activation Functions\n",
    "\n",
    "Now run the complete experiment suite. This will:\n",
    "1. Train 3 models for each activation function (5 activations × 3 trials = 15 models)\n",
    "2. Calculate reproducibility metrics (Relative PD)\n",
    "3. Save all results\n",
    "\n",
    "**Estimated time: 2-3 hours**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14fb733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiments for all activation functions\n",
    "all_results = {}\n",
    "all_models = {}\n",
    "\n",
    "activations_to_test = ['smelu_05', 'smelu_1', 'relu', 'gelu', 'swish']\n",
    "\n",
    "for activation_name in activations_to_test:\n",
    "    results, models, tokenizer = run_experiment(config, activation_name)\n",
    "    all_results[activation_name] = results\n",
    "    all_models[activation_name] = models\n",
    "    \n",
    "    # Brief pause between experiments\n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "print(\"\\n✓ All experiments completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f9ef5d",
   "metadata": {},
   "source": [
    "## Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e4904f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile summary statistics\n",
    "summary = []\n",
    "for activation_name, results in all_results.items():\n",
    "    summary.append({\n",
    "        'Activation': activation_name,\n",
    "        'Avg Val Loss': f\"{results['avg_val_loss']:.4f}\",\n",
    "        'Std Val Loss': f\"{results['std_val_loss']:.4f}\",\n",
    "        'Avg Relative PD': f\"{results['avg_relative_pd']:.6f}\",\n",
    "        'Avg Time (s)': f\"{results['avg_training_time']:.1f}\"\n",
    "    })\n",
    "\n",
    "import pandas as pd\n",
    "summary_df = pd.DataFrame(summary)\n",
    "print(\"\\nExperiment Summary:\")\n",
    "print(\"=\"*80)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4c88b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: Validation Loss vs Relative PD\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "activations = list(all_results.keys())\n",
    "val_losses = [all_results[act]['avg_val_loss'] for act in activations]\n",
    "relative_pds = [all_results[act]['avg_relative_pd'] for act in activations]\n",
    "\n",
    "# Create scatter plot\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(activations)))\n",
    "for i, act in enumerate(activations):\n",
    "    ax.scatter(relative_pds[i], val_losses[i], s=200, c=[colors[i]], \n",
    "               label=act.upper(), alpha=0.7, edgecolors='black', linewidth=1.5)\n",
    "\n",
    "ax.set_xlabel('Relative Prediction Difference (Lower = More Reproducible)', fontsize=12)\n",
    "ax.set_ylabel('Validation Loss (Lower = Better)', fontsize=12)\n",
    "ax.set_title('Reproducibility vs Accuracy Trade-off\\nCharacter-Level LM on Shakespeare', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='best', fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/reproducibility_vs_accuracy.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Plot saved to results/reproducibility_vs_accuracy.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2b1c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: Relative PD comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x = np.arange(len(activations))\n",
    "bars = ax.bar(x, relative_pds, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "ax.set_xlabel('Activation Function', fontsize=12)\n",
    "ax.set_ylabel('Average Relative Prediction Difference', fontsize=12)\n",
    "ax.set_title('Reproducibility Comparison Across Activation Functions\\n(Lower is Better)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([act.upper() for act in activations], rotation=45)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (bar, val) in enumerate(zip(bars, relative_pds)):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{val:.6f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/relative_pd_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Plot saved to results/relative_pd_comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc757fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: Validation loss comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "std_losses = [all_results[act]['std_val_loss'] for act in activations]\n",
    "bars = ax.bar(x, val_losses, yerr=std_losses, color=colors, alpha=0.7, \n",
    "              edgecolor='black', linewidth=1.5, capsize=5)\n",
    "\n",
    "ax.set_xlabel('Activation Function', fontsize=12)\n",
    "ax.set_ylabel('Average Validation Loss', fontsize=12)\n",
    "ax.set_title('Validation Loss Comparison Across Activation Functions\\n(Lower is Better)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([act.upper() for act in activations], rotation=45)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, val) in enumerate(zip(bars, val_losses)):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{val:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/val_loss_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Plot saved to results/val_loss_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d33735",
   "metadata": {},
   "source": [
    "## Sample Text Generation\n",
    "\n",
    "Generate text samples from each activation function to qualitatively compare outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2602c53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples from each activation function\n",
    "prompts = [\"ROMEO:\", \"JULIET:\", \"To be or not to be,\"]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    for activation_name, models in all_models.items():\n",
    "        model = models[0]  # Use first model from each activation\n",
    "        model.eval()\n",
    "        set_seed(42)\n",
    "        \n",
    "        context = torch.tensor(tokenizer.encode(prompt), dtype=torch.long).unsqueeze(0).to(config.device)\n",
    "        generated = model.generate(context, max_new_tokens=100, temperature=0.8)\n",
    "        generated_text = tokenizer.decode(generated[0].tolist())\n",
    "        \n",
    "        print(f\"[{activation_name.upper()}]\")\n",
    "        print(generated_text)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6dfd87",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "Analyze the results to answer:\n",
    "1. Which activation function provides the best reproducibility?\n",
    "2. Is there a trade-off between accuracy and reproducibility?\n",
    "3. Do smooth activations (SmeLU, Swish, GELU) show better reproducibility than ReLU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b273092c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best activation for reproducibility\n",
    "best_reproducibility = min(all_results.items(), key=lambda x: x[1]['avg_relative_pd'])\n",
    "print(f\"Best Reproducibility: {best_reproducibility[0].upper()}\")\n",
    "print(f\"  Relative PD: {best_reproducibility[1]['avg_relative_pd']:.6f}\")\n",
    "print(f\"  Val Loss: {best_reproducibility[1]['avg_val_loss']:.4f}\")\n",
    "\n",
    "# Find best activation for accuracy\n",
    "best_accuracy = min(all_results.items(), key=lambda x: x[1]['avg_val_loss'])\n",
    "print(f\"\\nBest Accuracy: {best_accuracy[0].upper()}\")\n",
    "print(f\"  Val Loss: {best_accuracy[1]['avg_val_loss']:.4f}\")\n",
    "print(f\"  Relative PD: {best_accuracy[1]['avg_relative_pd']:.6f}\")\n",
    "\n",
    "# Compare smooth vs non-smooth activations\n",
    "smooth = ['smelu_05', 'smelu_1', 'gelu', 'swish']\n",
    "non_smooth = ['relu']\n",
    "\n",
    "smooth_avg_pd = np.mean([all_results[act]['avg_relative_pd'] for act in smooth if act in all_results])\n",
    "non_smooth_avg_pd = np.mean([all_results[act]['avg_relative_pd'] for act in non_smooth if act in all_results])\n",
    "\n",
    "print(f\"\\nSmooth Activations Avg PD: {smooth_avg_pd:.6f}\")\n",
    "print(f\"Non-Smooth Activations Avg PD: {non_smooth_avg_pd:.6f}\")\n",
    "print(f\"\\n{'Smooth activations are MORE reproducible!' if smooth_avg_pd < non_smooth_avg_pd else 'ReLU is MORE reproducible!'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e26032",
   "metadata": {},
   "source": [
    "## Save Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceeb6694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save complete summary\n",
    "final_summary = {\n",
    "    'config': config.__dict__,\n",
    "    'results': {k: v for k, v in all_results.items()},\n",
    "    'best_reproducibility': {\n",
    "        'activation': best_reproducibility[0],\n",
    "        'relative_pd': best_reproducibility[1]['avg_relative_pd'],\n",
    "        'val_loss': best_reproducibility[1]['avg_val_loss']\n",
    "    },\n",
    "    'best_accuracy': {\n",
    "        'activation': best_accuracy[0],\n",
    "        'val_loss': best_accuracy[1]['avg_val_loss'],\n",
    "        'relative_pd': best_accuracy[1]['avg_relative_pd']\n",
    "    },\n",
    "    'smooth_vs_nonsmooth': {\n",
    "        'smooth_avg_pd': smooth_avg_pd,\n",
    "        'nonsmooth_avg_pd': non_smooth_avg_pd\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('results/final_summary.json', 'w') as f:\n",
    "    json.dump(final_summary, f, indent=2)\n",
    "\n",
    "print(\"Final summary saved to results/final_summary.json\")\n",
    "print(\"\\n✓ Experiment complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
